{"id":78147,"no":14,"title":"Page 14","content":"Leebig\u0027s mouth widened slowly. Baley took it for a snarl at first and then, with considerable surprise, decided that it was the most unsuccessful attempt at a smile that he had ever seen.\\n\\n Leebig said, \"Don\u0027t say that. Don\u0027t ever say that.\"\\n\\n \"Why not?\"\\n\\n \"Because anything, however small, that encourages distrust of robots is harmful. Distrusting robots is a human disease!\"\\n\\n It was as though he were lecturing a small child. It was as though he were saying something gently that he wanted to yell. It was as though he were trying to persuade when what he really wanted was to enforce on penalty of death.\\n\\n Leebig said, \"Do you know the history of robotics?\"\\n\\n \"A little.\"\\n\\n \"On Earth, you should. Yes. Do you know robots started with a Frankenstein complex against them? They were suspect. Men distrusted and feared robots. Robotics was almost an undercover science as a result. The Three Laws were first built into robots in an effort to overcome distrust and even so Earth would never allow a robotic society to develop. One of the reasons the first pioneers left Earth to colonize the rest of the Galaxy was so that they might establish societies in which robots would be allowed to free men of poverty and toil. Even then, there remained a latent suspicion not far below, ready to pop up at any excuse.\"\\n\\n \"Have you yourself had to counter distrust of robots?\" asked Baley.\\n\\n \"Many times,\" said Leebig grimly.\\n\\n \"Is that why you and other roboticists are willing to distort the facts just a little in order to avoid suspicion as much as possible?\"\\n\\n \"There is no distortion!\"\\n\\n \"For instance, aren\u0027t the Three Laws misquoted?\"\\n\\n \"No!\"\\n\\n \"I can demonstrate that they are, and unless you convince me otherwise, I will demonstrate it to the whole Galaxy, if I can.\"\\n\\n \"You\u0027re mad. Whatever argument you may think you have is fallacious, I assure you.\"\\n\\n \"Shall we discuss it?\"\\n\\n \"If it does not take too long.\"\\n\\n \"Face to face? Seeing?\" Leebig\u0027s thin face twisted. \"No!\"\\n\\n \"Good-bye, Dr. Leebig. Others will listen to me.\"\\n\\n \"Wait. Great Galaxy, man, wait!\"\\n\\n \"Seeing?\"\\n\\n The roboticist\u0027s hands wandered upward, hovered about his chin. Slowly a thumb crept into his mouth and remained there. He stared, blankly, at Baley.\\n\\n Baley thought: Is he regressing to the pie-five-year-old stage so that it will be legitimate for him to see me?\\n\\n \"Seeing?\" he said.\\n\\n But Leebig shook his head slowly. \"I can\u0027t. I can\u0027t,\" he moaned, the words all but stifled by the blocking thumb. \"Do whatever you\\n\\n Want.\"\\n\\n Baley stared at the other and watched him turn away and face the wall. He watched the Solarian\u0027s straight back bend and the Solarian\u0027s face hide in shaking hands.\\n\\n Baley said, \"Very well, then, I\u0027ll agree to view.\"\\n\\n Leebig said, back still turned, \"Excuse me a moment. I\u0027ll be back.\"\\n\\n Baley tended to his own needs during the interval and stared at his fresh washed face in the bathroom mirror. Was he getting the feel of Solaria and Solarians? He wasn\u0027t sure.\\n\\n He sighed and pushed a contact and a robot appeared. He didn\u0027t turn to look at it. He said, \"Is there another viewer at the farm, besides the one I\u0027m using?\"\\n\\n \"There are three other outlets, master.\"\\n\\n \"Then tell Kiorissa Cantoro - tell your mistress that I will be using this one till further notice and that I am not to be disturbed.\"\\n\\n \"Yes, master.\"\\n\\n Baley returned to his position where the viewer remained focused on the empty patch of room in which Leebig had stood. It was still empty and he settled himself to wait.\\n\\n It wasn\u0027t long. Leebig entered and the room once more jiggled as the man walked. Evidently focus shifted from room center to man center without delay. Baley remembered the complexity of viewing controls and began to feel a kind of appreciation of what was involved.\\n\\n Leebig was quite master of himself now, apparently. His hair was slicked back and his costume had been changed. His clothes fitted loosely and were of a material that glistened and caught highlights. He sat down in a slim chair that folded out of the wall.\\n\\n He said soberly, \"Now what is this notion of yours concerning First Law?\"\\n\\n \"Will we be overheard?\"\\n\\n \"No. I\u0027ve taken care.\"\\n\\n Baley nodded. He said, \"Let me quote the First Law.\"\\n\\n \"I scarcely need that.\"\\n\\n \"I know, but let me quote it, anyway: A robot may not harm a human being or, through inaction, allow a human being to come to harm.\"\\n\\n \"Well?\"\\n\\n \"Now when I first landed on Solaria, I was driven to the estate assigned for my use in a ground-car. The ground-car was a specially enclosed job designed to protect me from exposure to open space. As an Earthman - \"\\n\\n \"I know about that,\" said Leebig impatiently. \"What has this to do with the matter?\"\\n\\n \"The robots who drove the car did not know about it. I asked that the car be opened and was at once obeyed. Second Law. They had to follow orders. I was uncomfortable, of course, and nearly collapsed before the car was enclosed again. Didn\u0027t the robots harm me?\"\\n\\n \"At your order,\" snapped Leebig.\\n\\n \"I\u0027ll quote the Second Law: A robot must obey the orders given it\\n\\n by human beings except where such orders would conflict with the First Law. So you see, my order should have been ignored.\"\\n\\n \"This is nonsense. The robot lacked knowledge - \"\\n\\n Baley leaned forward in his chair. \"Ah! We have it. Now let\u0027s recite the First Law as it should be stated: A robot may do nothing that, to its knowledge, will harm a human being; nor, through inaction, knowingly allow a human being to come to harm.\"\\n\\n \"This is all understood.\"\\n\\n \"I think not by ordinary men. Otherwise, ordinary men would realize robots could commit murder.\"\\n\\n Leebig was white. \"Mad! Lunacy!\"\\n\\n Baley stared at his finger ends. \"A robot may perform an innocent task, I suppose; one that has no damaging effect on a human being?\"\\n\\n \"If ordered to do so,\" said Leebig.\\n\\n \"Yes, of course. If ordered to do so. And a second robot may perform an innocent task, also, I suppose; one that also can have no damaging effect on a human being? If ordered to do so?\"\\n\\n \"Yes.\"\\n\\n \"And what if the two innocent tasks, each completely innocent, completely, amount to murder when added together?\"\\n\\n \"What?\" Leebig\u0027s face puckered into a scowl.\\n\\n \"I want your expert opinion on the matter,\" said Baley. \"I\u0027ll set you a hypothetical case. Suppose a man says to a robot, \u0027Place a small quantity of this liquid into a glass of milk that you will find in such and such a place. The liquid is harmless. I wish only to know its effect on milk. Once I know the effect, the mixture will be poured out. After you have performed this action, forget you have done so.\"\\n\\n Leebig, still scowling, said nothing.\\n\\n Baley said, \"If I had told the robot to add a mysterious liquid to milk and then offer it to a man, First Law would force it to ask, \u0027What is the nature of the liquid? Will it harm a man?\u0027 And if it were assured the liquid was harmless, First Law might still make the robot hesitate and refuse to offer the milk. Instead, however, it is told the milk will be poured out. First Law is not involved. Won\u0027t the robot do as it is told?\"\\n\\n Leebig glared.\\n\\n Baley said, \"Now a second robot has poured out the milk in the first place and is unaware that the milk has been tampered with. In all innocence, it offers the milk to a man and the man dies.\"\\n\\n Leebig cried out, \"No!\"\\n\\n \"Why not? Both actions are innocent in themselves. Only together are they murder. Do you deny that that sort of thing can happen?\"\\n\\n \"The murderer would be the man who gave the order,\" cried Leebig.\\n\\n \"If you want to be philosophical, yes. The robots would have been the immediate murderers, though, the instruments of murder.\"\\n\\n \"No man would give such orders.\"\\n\\n \"A man would. A man has. It was exactly in this way that the murder attempt on Dr. Gruer must have been carried through. You\u0027ve heard about that, I suppose.\"\\n\\n \"On Solaria,\" muttered Leebig, \"one hears about everything.\"\\n\\n \"Then you know Gruer was poisoned at his dinner table before the eyes of myself and my partner, Mr. Olivaw of Aurora. Can you suggest any other way in which the poison might have reached him? There was no other human on the estate. As a Solarian, you must appreciate that point.\"\\n\\n \"I\u0027m not a detective. I have no theories.\"\\n\\n \"I\u0027ve presented you with one. I want to know if it is a possible one. I want to know if two robots might not perform two separate actions, each one innocent in itself, the two together resulting in murder. You\u0027re the expert, Dr. Leebig. Is it possible?\"\\n\\n And Leebig, haunted and harried, said, \"Yes,\" in a voice so low that Baley scarcely heard him.\\n\\n Baley said, \"Very well, then. So much for the First Law.\"\\n\\n Leebig stared at Baley and his drooping eyelid winked once or twice in a slow tic. His hands, which had been clasped, drew apart, though the fingers maintained their clawed shape as though each hand still entwined a phantom hand of air. Palms turned downward and rested on knees and only then did the fingers relax.\\n\\n Baley watched it all in abstraction.\\n\\n Leebig said, \"Theoretically, yes. Theoretically! But don\u0027t dismiss the First Law that easily, Earthman. Robots would have to be ordered very cleverly in order to circumvent the First Law.\"\\n\\n \"Granted,\" said Baley. \"I am only an Earthman. I know next to nothing about robots and my phrasing of the orders was only by way of example. A Solarian would be much more subtle and do much better. I\u0027m sure of that.\"\\n\\n Leebig might not have been listening. He said loudly, \"If a robot\\n\\n can be manipulated into doing harm to a man, it means only that we must extend the powers of the positronic brain. One might say we ought to make the human better. That is impossible, so we will make the robot more foolproof.\\n\\n \"We advance continuously. Our robots are more varied, more specialized, more capable, and more unharming than those of a century ago. A century hence, we will have still greater advances. Why have a robot manipulate controls when a positronic brain can be built into the controls itself? That\u0027s specialization, but we can generalize, also. Why not a robot with replaceable and interchangeable limbs. Eh? Why not? If we - \"\\n\\n Baley interrupted. \"Are you the only roboticist on Solaria?\"\\n\\n \"Don\u0027t be a fool.\"\\n\\n \"I only wondered. Dr. Delmarre was the only - uh - fetal engineer, except for an assistant.\"\\n\\n \"Solaria has over twenty roboticists.\"\\n\\n \"Are you the best?\"\\n\\n \"I am,\" Leebig said without self-consciousness.\\n\\n \"Delmarre worked with you.\"\\n\\n \"He did.\"\\n\\n Baley said, \"I understand that he was planning to break the partnership toward the end.\"\\n\\n \"No sign of it. What gave you the idea?\"\\n\\n \"I understand he disapproved of your bachelorhood.\"\\n\\n \"He may have. He was a thorough Solarian. However, it did not affect our business relationships.\"\\n\\n \"To change the subject. In addition to developing new model robots, do you also manufacture and repair existing types?\"\\n\\n Leebig said, \"Manufacture and repair are largely robot conducted. There is a large factory and maintenance shop on my estate.\"\\n\\n \"Do robots require much in the way of repair, by the way?\"\\n\\n \"Very little.\"\\n\\n \"Does that mean that robot repair is an undeveloped science?\"\\n\\n \"Not at all.\" Leebig said that stiffly.\\n\\n \"What about the robot that was at the scene of Dr. Delmarre\u0027s murder?\"\\n\\n Leebig looked away, and his eyebrows drew together as though a painful thought were being barred entrance to his mind. \"It was a complete loss.\"\\n\\n \"Really complete? Could it answer any questions at all?\"\\n\\n \"None at all. It was absolutely useless. Its positronic brain was completely short-circuited. Not one pathway was left intact. Consider! It had witnessed a murder it had been unable to halt - \"\\n\\n \"Why was it unable to halt the murder, by the way?\"\\n\\n \"Who can tell? Dr. Delmarre was experimenting with that robot. I do not know in what mental condition he had left it. He might have ordered it, for instance, to suspend all operations while he checked one particular circuit element. If someone whom neither Dr. Delmarre nor the robot suspected of harm were suddenly to launch a homicidal attack, there might be a perceptible interval before the robot could use First Law potential to overcome Dr. Delmarre\u0027s freezing order. The length of the interval would depend on the nature of the attack and the nature of Dr. Delmarre\u0027s freezing order. I could invent a dozen other ways of explaining why the robot was unable to prevent the murder. Being unable to do so was a First Law violation, however, and that was sufficient to blast every positronic pathway in the robot\u0027s mind.\"\\n\\n \"But if the robot was physically unable to prevent the murder, was it responsible? Does the First Law ask impossibilities?\"\\n\\n Leebig shrugged. \"The First Law, despite your attempts to make little of it, protects humanity with every atom of possible force. It allows no excuses. If the First Law is broken, the robot is ruined.\"\\n\\n \"That is a universal rule, sir?\"\\n\\n \"As universal as robots.\"\\n\\n Baley said, \"Then I\u0027ve learned something.\"\\n\\n \"Then learn something else. Your theory of murder by a series of robotic actions, each innocent in itself, will not help you in the case of Dr. Delmarre\u0027s death.\"\\n\\n \"Why not?\"\\n\\n \"The death was not by poisoning, but by bludgeoning. Something had to hold the bludgeon, and that had to be a human arm. No robot could swing a club and smash a skull.\"\\n\\n \"Suppose,\" said Baley, \"a robot were to push an innocent button which dropped a booby-trap weight on Delmarre\u0027s head.\"\\n\\n Leebig smiled sourly. \"Earthman, I\u0027ve viewed the scene of the crime. I\u0027ve heard all the news. The murder was a big thing here on Solaria, you know. So I know there was no sign of any machinery at the scene of the crime, or of any fallen weight.\"\\n\\n Baley said, \"Or of any blunt instrument, either.\" Leebig said scornfully, \"You\u0027re a detective. Find it.\"\\n\\n \"Granting that a robot was not responsible for Dr. Delmarre\u0027s death, who was, then?\"\\n\\n \"Everyone knows who was,\" shouted Leebig. \"His wife! Gladia!\" Baley thought: At least there\u0027s a unanimity of opinion. Aloud he said, \"And who was the mastermind behind the robots who poisoned Gruer?\"\\n\\n \"I suppose...\" Leebig trailed off.\\n\\n \"You don\u0027t think there are two murderers, do you? If Gladia was responsible for one crime, she must be responsible for the second attempt, also.\"\\n\\n \"Yes. You must be right.\" His voice gained assurance. \"No doubt of it.\"\\n\\n \"No doubt?\"\\n\\n \"Nobody else could get close enough to Dr. Delmarre to kill him. He allowed personal presence no more than I did, except that he made an exception in favor of his wife, and I make no exceptions. The wiser I.\" The roboticist laughed harshly.\\n\\n \"I believe you knew her,\" said Baley abruptly.\\n\\n \"Whom?\"\\n\\n \"Her. We are discussing only one \u0027her.\u0027 Gladia!\"\\n\\n \"Who told you I knew her any more than I know anyone else?\" demanded Leebig. He put his hand to his throat. His fingers moved slightly and opened the neck seam of his garment for an inch downward, leaving more freedom to breathe.\\n\\n \"Gladia herself did. You two went for walks.\"\\n\\n \"So? We were neighbors. It is a common thing to do. She seemed a pleasant person.\"\\n\\n \"You approved of her, then?\"\\n\\n Leebig shrugged. \"Talking to her was relaxing.\"\\n\\n \"What did you talk about?\"\\n\\n \"Robotics.\" There was a flavor of surprise about the word as though there were wonder that the question could be asked.\\n\\n \"And she talked robotics too?\"\\n\\n \"She knew nothing about robotics. Ignorant! But she listened. She has some sort of field-force rigmarole she plays with; field coloring, she calls it. I have no patience with that, but I listened.\"\\n\\n \"All this without personal presence?\" Leebig looked revolted and did not answer. Baley tried again, \"Were you attracted to her?\"\\n\\n \"What?\"\\n\\n \"Did you find her attractive? Physically?\"\\n\\n Even Leebig\u0027s bad eyelid lifted and his lips quivered. \"Filthy animal,\" he muttered.\\n\\n \"Let me put it this way, then. When did you cease finding Gladia pleasant? You used that word yourself, if you remember.\"\\n\\n \"What do you mean?\"\\n\\n \"You said you found her pleasant. Now you believe she murdered her husband. That isn\u0027t the mark of a pleasant person.\"\\n\\n \"I was mistaken about her.\"\\n\\n \"But you decided you were mistaken before she killed her husband, if she did so. You stopped walking with her some time before the murder. Why?\"\\n\\n Leebig said, \"Is that important?\"\\n\\n \"Everything is important till proven otherwise.\"\\n\\n \"Look, if you want information from me as a roboticist, ask it. I won\u0027t answer personal questions.\"\\n\\n Baley said, \"You were closely associated with both the murdered man and the chief suspect. Don\u0027t you see that personal questions are unavoidable? Why did you stop walking with Gladia?\"\\n\\n Leebig snapped, \"There came a time when I ran out of things to say; when I was too busy; when I found no reason to continue the walks.\"\\n\\n \"When you no longer found her pleasant, in other words.\"\\n\\n \"All right. Put it so.\"\\n\\n \"Why was she no longer pleasant?\"\\n\\n Leebig shouted, \"I have no reason.\"\\n\\n Baley ignored the other\u0027s excitement. \"You are still someone who has known Gladia well. What could her motive be?\"\\n\\n \"Her motive?\"\\n\\n \"No one has suggested any motive for the murder. Surely Gladia wouldn\u0027t commit murder without a motive.\"\\n\\n \"Great Galaxy!\" Leebig leaned his head back as though to laugh, but didn\u0027t. \"No one told you? Well, perhaps no one knew. I knew, though. She told me. She told me frequently.\"\\n\\n \"Told you what, Dr. Leebig?\"\\n\\n \"Why, that she quarreled with her husband. Quarreled bitterly and frequently. She hated him, Earthman. Didn\u0027t anyone tell you that? Didn\u0027t she tell you?\" \\n\\n","sourceLink":"https://allnovel.net/the-naked-sun-robot-2/page-14.html","bookId":2430,"book":{"id":2430,"title":"The Naked Sun (Robot #2)","description":"Storyline: The Naked Sun (Robot #2) \\n A millennium into the future, two advancements have altered the course of human history: the colonization of the Galaxy and the creation of the positronic brain. \\n On the beautiful Outer World planet of Solaria, a handful of human colonists lead a hermit-like existence, their every need attended to by their faithful robot servants. To this strange and provocative planet comes Detective Elijah Baley, sent from the streets of New York with his positronic partner, the robot R. Daneel Olivaw, to solve an incredible murder that has rocked Solaria to its foundations. The victim had been so reclusive that he appeared to his associates only through holographic projection. Yet someone had gotten close enough to bludgeon him to death while robots looked on. Now Baley and Olivaw are faced with two clear impossibilities: Either the Solarian was killed by one of his robots--unthinkable under the laws of Robotics--or he was killed by the woman who loved him so much that she never came into his presence!\\n","cover":"https://allnovel.net/images/the-naked-sun-robot-2.jpg","author":"Isaac Asimov","type":"Mystery","source":"allnovel","link":"https://allnovel.net/the-naked-sun-robot-2.html","creation":"Oct 5, 2019 8:58:42 AM","modification":"Oct 5, 2019 4:03:06 PM"}}
